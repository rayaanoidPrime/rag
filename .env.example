# --- General Configuration ---
APP_TITLE="ðŸš€ RAG with txtai & Advanced Parsing"
LOG_LEVEL="DEBUG" # DEBUG, INFO, WARNING, ERROR

# --- LLM Configuration ---
# Options:
# "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4" (for x86_64/AMD with CUDA)
# "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" (for CPU or other platforms)
# Or any other txtai compatible model string
LLM_MODEL="hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
# LLM_MODEL="bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
LLM_MAX_LENGTH=4096
TOPICS_BATCH_SIZE=8 # Batch size for LLM topic generation, set to 0 to disable batching

# --- Embeddings Configuration ---
EMBEDDINGS_PATH="embeddings_db" # Local path to save/load embeddings
# EMBEDDINGS_PATH="neuml/txtai-wikipedia-slim" # Example Hugging Face Hub model
EMBEDDINGS_MODEL_PATH="intfloat/e5-large"
EMBEDDINGS_CONTENT_ENABLED=true
EMBEDDINGS_INSTRUCTIONS_QUERY="query: "
EMBEDDINGS_INSTRUCTIONS_DATA="passage: "
PERSIST_EMBEDDINGS_PATH="embeddings_db_persisted" # Set to a path to persist, leave empty to not persist after initial load/creation unless new data is added

# --- Initial Data Configuration ---
# DATA_PATH="./sample_data" # Optional: Path to a directory with initial documents to index

# --- RAG Configuration ---
RAG_CONTEXT_SIZE=10
RAG_SYSTEM_PROMPT="You are a friendly assistant. You answer questions from users based on the provided context."
RAG_TEMPLATE="""
Answer the following question using only the context below. Only include information
specifically discussed.

question: {question}
context: {context} """

# --- GraphRAG Configuration ---
GRAPH_ENABLED=true
GRAPH_APPROXIMATE=false
GRAPH_MIN_SCORE=0.7
GRAPH_CONTEXT_SIZE=10 # Max nodes to return from graph path for context

# --- Textractor Configuration ---
# Options: "available", "pdfminer", "pdfplumber", "pymupdf", "unstructured", "ocr" etc.
# "available" tries to pick the best available.
TEXTRACTOR_BACKEND="available"
TEXTRACTOR_PARAGRAPHS=true

# --- UI Examples ---
# Semicolon-separated example queries
UI_EXAMPLES="Who created Linux?;gq: Tell me about Linux;linux -> macos -> microsoft windows;linux -> macos -> microsoft windows gq: Tell me about Linux"

# --- Torch / CUDA Configuration (if applicable) ---
# CUDA_VISIBLE_DEVICES="0" # If you need to specify GPUs

# Note: Boolean values are typically interpreted as strings by dotenv.
# The application code (e.g. Pydantic) should handle casting to bool.
DB_HOST=localhost
DB_PORT=5432
DB_USER=your_db_user
DB_PASSWORD=your_db_password
DB_NAME=rag_app_db
